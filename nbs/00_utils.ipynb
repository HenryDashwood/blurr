{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "\n",
    "> Various utility functions used by the blurr package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import sys, inspect\n",
    "from enum import Enum\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "from fastai.text.all import *\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pytorch 1.7.1\nUsing fastai 2.1.8\nUsing transformers 4.0.1\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import pdb\n",
    "\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "\n",
    "from fastai import __version__ as fa_version\n",
    "from torch import __version__ as pt_version\n",
    "from transformers import __version__ as hft_version\n",
    "\n",
    "print(f'Using pytorch {pt_version}')\n",
    "print(f'Using fastai {fa_version}')\n",
    "print(f'Using transformers {hft_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #cuda\n",
    "# torch.cuda.set_device(1)\n",
    "# print(f'Using GPU #{torch.cuda.current_device()}: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def str_to_class(classname):\n",
    "    \"converts string representation to class\"\n",
    "    return getattr(sys.modules[__name__], classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Singleton:\n",
    "    def __init__(self,cls):\n",
    "        self._cls, self._instance = cls, None\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self._instance == None: self._instance = self._cls(*args, **kwargs)\n",
    "        return self._instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Singleton` functions as python decorator.  Use this above any class to turn that class into a singleton (see [here](https://python-3-patterns-idioms-test.readthedocs.io/en/latest/Singleton.html) for more info on the singleton pattern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Singleton\n",
    "class TestSingleton: pass\n",
    "\n",
    "a = TestSingleton()\n",
    "b = TestSingleton()\n",
    "test_eq(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@Singleton\n",
    "class ModelHelper():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # get hf classes (tokenizers, configs, models, etc...)\n",
    "        transformer_classes = inspect.getmembers(sys.modules[__name__], \n",
    "                                                 lambda member: inspect.isclass(member)\n",
    "                                                 and member.__module__.startswith('transformers.'))\n",
    "        \n",
    "        # build a df that we can query against to get various transformers objects/info\n",
    "        self._df = pd.DataFrame(transformer_classes, columns=['class_name', 'class_location'])\n",
    "        \n",
    "        # add the module each class is included in\n",
    "        self._df['module'] = self._df.class_location.apply(lambda v: v.__module__)\n",
    "        \n",
    "        # remove class_location (don't need it anymore)\n",
    "        self._df.drop(labels=['class_location'], axis=1, inplace=True)\n",
    "        \n",
    "        # break up the module into separate cols\n",
    "        module_parts_df = self._df.module.str.split(\".\", n = -1, expand = True) \n",
    "        for i in range(len(module_parts_df.columns)):\n",
    "            self._df[f'module_part_{i}'] = module_parts_df[i]\n",
    "\n",
    "        # using module part 3, break up the functional area and arch into separate cols\n",
    "        module_part_3_df = self._df.module_part_3.str.split(\"_\", n = 1, expand = True) \n",
    "        self._df[['functional_area', 'arch']] = module_part_3_df\n",
    "        \n",
    "        # if functional area = modeling, pull out the task it is built for\n",
    "        model_type_df = self._df[(self._df.functional_area == 'modeling')].class_name.str.split('For', n=1, expand=True)\n",
    "        \n",
    "        model_type_df[1] = np.where(model_type_df[1].notnull(), \n",
    "                                    'For' + model_type_df[1].astype(str), \n",
    "                                    model_type_df[1])\n",
    "        \n",
    "        self._df['model_task'] = model_type_df[1]\n",
    "        self._df['model_task'] = self._df['model_task'].str.replace('For', '', n=1, case=True, regex=False)\n",
    "        \n",
    "        model_type_df = self._df[(self._df.functional_area == 'modeling')].class_name.str.split('With', n=1, expand=True)\n",
    "        model_type_df[1] = np.where(model_type_df[1].notnull(), \n",
    "                                    'With' + model_type_df[1].astype(str), \n",
    "                                    self._df[(self._df.functional_area == 'modeling')].model_task)\n",
    "        \n",
    "        self._df['model_task'] = model_type_df[1]\n",
    "        self._df['model_task'] = self._df['model_task'].str.replace('With', '', n=1, case=True, regex=False)\n",
    "        \n",
    "        # look at what we're going to remove (use to verify we're just getting rid of stuff we want too)\n",
    "        # df[~df['hf_class_type'].isin(['modeling', 'configuration', 'tokenization'])]\n",
    "        \n",
    "        # only need these 3 functional areas for our querying purposes\n",
    "        self._df = self._df[self._df['functional_area'].isin(['modeling', 'configuration', 'tokenization'])]\n",
    "        \n",
    "    def get_architectures(self): \n",
    "        \"\"\"Used to get all the architectures supported by your `Transformers` install\"\"\"\n",
    "        return sorted(self._df[(self._df.arch.notna()) & \n",
    "                        (self._df.arch != None) & \n",
    "                        (self._df.arch != 'utils')].arch.unique().tolist())\n",
    "    \n",
    "    def get_config(self, arch): \n",
    "        \"\"\"Used the locate the name of the configuration class for a given architecture\"\"\"\n",
    "        config = self._df[(self._df.functional_area == 'configuration') & \n",
    "                          (self._df.arch == arch)].class_name.values[0]\n",
    "        \n",
    "        return str_to_class(config)\n",
    "    \n",
    "    def get_tokenizers(self, arch): \n",
    "        \"\"\"Used to get the available huggingface tokenizers for a given architecture. Note: There may be \n",
    "        multiple tokenizers and so this returns a list.\n",
    "        \"\"\"\n",
    "        toks = sorted(self._df[(self._df.functional_area == 'tokenization') & \n",
    "                               (self._df.arch == arch)].class_name.values)\n",
    "        \n",
    "        return [str_to_class(tok_name) for tok_name in toks]\n",
    "    \n",
    "    def get_tasks(self, arch=None): \n",
    "        \"\"\"Get the type of tasks for which there is a custom model for (*optional: by architecture*). \n",
    "        There are a number of customized models built for specific tasks like token classification, \n",
    "        question/answering, LM, etc....\n",
    "        \"\"\"\n",
    "        query = ['model_task.notna()']\n",
    "        if (arch): query.append(f'arch == \"{arch}\"')\n",
    "\n",
    "        return sorted(self._df.query(' & '.join(query), engine='python').model_task.unique().tolist())\n",
    "    \n",
    "    def get_models(self, arch=None, task=None):\n",
    "        \"\"\"The transformer models available for use (optional: by architecture | task)\"\"\"\n",
    "        query = ['functional_area == \"modeling\"']\n",
    "        if (arch): query.append(f'arch == \"{arch}\"')\n",
    "        if (task): query.append(f'model_task == \"{task}\"')\n",
    "\n",
    "        models = sorted(self._df.query(' & '.join(query)).class_name.tolist())\n",
    "        return [str_to_class(model_name) for model_name in models] \n",
    "    \n",
    "    def get_classes_for_model(self, model_name_or_cls):\n",
    "        \"\"\"Get tokenizers, config, and model for a given model name / class\"\"\"\n",
    "        model_name = model_name_or_cls if isinstance(model_name_or_cls, str) else model_name_or_cls.__name__\n",
    "\n",
    "        meta = self._df[self._df.class_name == model_name]\n",
    "        tokenizers = self.get_tokenizers(meta.arch.values[0])\n",
    "        config = self.get_config(meta.arch.values[0])\n",
    "\n",
    "        return (config, tokenizers, str_to_class(model_name))\n",
    "    \n",
    "    def get_model_architecture(self, model_name_or_enum):\n",
    "        \"\"\"Get the architecture for a given model name / enum\"\"\"\n",
    "        model_name = model_name_or_enum if isinstance(model_name_or_enum, str) else model_name_or_enum.name\n",
    "        return self._df[self._df.class_name == model_name].arch.values[0]\n",
    "    \n",
    "    def get_hf_objects(self, pretrained_model_name_or_path, task=None,\n",
    "                       config=None, tokenizer_cls=None, model_cls=None, \n",
    "                       config_kwargs={}, tokenizer_kwargs={}, model_kwargs={}, cache_dir=None):\n",
    "        \"\"\"Returns the architecture (str), config (obj), tokenizer (obj), and model (obj) given at minimum a\n",
    "        `pre-trained model name or path`. Specify a `task` to ensure the right \"AutoModelFor<task>\" is used to\n",
    "        create the model.\n",
    "        \n",
    "        Optionally, you can pass a config (obj), tokenizer (class), and/or model (class) (along with any \n",
    "        related kwargs for each) to get as specific as you want w/r/t what huggingface objects are returned.\n",
    "        \"\"\"\n",
    "        \n",
    "        # config\n",
    "        if (config is None):\n",
    "            config = AutoConfig.from_pretrained(pretrained_model_name_or_path, cache_dir=cache_dir, **config_kwargs)\n",
    "            \n",
    "        # tokenizer\n",
    "        if (tokenizer_cls is None):\n",
    "            tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, \n",
    "                                                      cache_dir=cache_dir, \n",
    "                                                      **tokenizer_kwargs)\n",
    "        else:\n",
    "            tokenizer = tokenizer_cls.from_pretrained(pretrained_model_name_or_path, \n",
    "                                                      cache_dir=cache_dir, \n",
    "                                                      **tokenizer_kwargs)\n",
    "            \n",
    "        # model\n",
    "        if (model_cls is None and task is None):\n",
    "            model = AutoModel.from_pretrained(pretrained_model_name_or_path, \n",
    "                                              config=config, \n",
    "                                              cache_dir=cache_dir, \n",
    "                                              **model_kwargs)\n",
    "        else:\n",
    "            if (model_cls is None and task is not None): \n",
    "                model_cls = self.get_models(arch=\"auto\", task=task.name)[0]\n",
    "            \n",
    "            model = model_cls.from_pretrained(pretrained_model_name_or_path, \n",
    "                                              config=config, \n",
    "                                              cache_dir=cache_dir, \n",
    "                                              **model_kwargs)\n",
    "            \n",
    "        #arch\n",
    "        arch = self.get_model_architecture(type(model).__name__)\n",
    "        \n",
    "        return (arch, config, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ModelHelper` is a `Singleton` (there exists only one instance, and the same instance is returned upon subsequent instantiation requests).  You can get at via the `BLURR_MODEL_HELPER` constant below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh = ModelHelper()\n",
    "# mh2 = ModelHelper()\n",
    "# test_eq(mh, mh2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class_name</th>\n      <th>module</th>\n      <th>module_part_0</th>\n      <th>module_part_1</th>\n      <th>module_part_2</th>\n      <th>module_part_3</th>\n      <th>functional_area</th>\n      <th>arch</th>\n      <th>model_task</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>AdaptiveEmbedding</td>\n      <td>transformers.models.transfo_xl.modeling_transfo_xl</td>\n      <td>transformers</td>\n      <td>models</td>\n      <td>transfo_xl</td>\n      <td>modeling_transfo_xl</td>\n      <td>modeling</td>\n      <td>transfo_xl</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AlbertConfig</td>\n      <td>transformers.models.albert.configuration_albert</td>\n      <td>transformers</td>\n      <td>models</td>\n      <td>albert</td>\n      <td>configuration_albert</td>\n      <td>configuration</td>\n      <td>albert</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>AlbertForMaskedLM</td>\n      <td>transformers.models.albert.modeling_albert</td>\n      <td>transformers</td>\n      <td>models</td>\n      <td>albert</td>\n      <td>modeling_albert</td>\n      <td>modeling</td>\n      <td>albert</td>\n      <td>MaskedLM</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>AlbertForMultipleChoice</td>\n      <td>transformers.models.albert.modeling_albert</td>\n      <td>transformers</td>\n      <td>models</td>\n      <td>albert</td>\n      <td>modeling_albert</td>\n      <td>modeling</td>\n      <td>albert</td>\n      <td>MultipleChoice</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>AlbertForPreTraining</td>\n      <td>transformers.models.albert.modeling_albert</td>\n      <td>transformers</td>\n      <td>models</td>\n      <td>albert</td>\n      <td>modeling_albert</td>\n      <td>modeling</td>\n      <td>albert</td>\n      <td>PreTraining</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, nan, 'MaskedLM', 'MultipleChoice', 'PreTraining', 'QuestionAnswering', 'SequenceClassification', 'TokenClassification', 'CausalLM', 'NextSentencePrediction', 'Seq2SeqLM', 'LMHead', 'ConditionalGeneration', 'QuestionAnsweringSimple', 'LMHeadModel', 'Classification', 'Generation']\n\n['modeling', 'configuration', 'tokenization']\n\n['transfo_xl', 'albert', 'auto', 'bart', 'bert', 'bert_generation', 'bert_japanese', 'bertweet', 'blenderbot', 'ctrl', 'camembert', 'dpr', 'deberta', 'distilbert', 'electra', 'encoder_decoder', 'fsmt', 'flaubert', 'funnel', 'gpt2', 'herbert', 'layoutlm', 'longformer', 'lxmert', 'mbart', 'mmbt', 'mt5', 'marian', 'mobilebert', 'openai', 'pegasus', 'phobert', 'prophetnet', 'rag', 'reformer', 'retribert', 'roberta', 'squeezebert', 't5', 'xlm', 'xlm_roberta', 'xlnet', 'xlm_prophetnet']\n\n['modeling_transfo_xl', 'configuration_albert', 'modeling_albert', 'tokenization_albert', 'tokenization_albert_fast', 'configuration_auto', 'modeling_auto', 'tokenization_auto', 'configuration_bart', 'modeling_bart', 'tokenization_bart', 'tokenization_bart_fast', 'tokenization_bert', 'configuration_bert', 'modeling_bert', 'configuration_bert_generation', 'modeling_bert_generation', 'tokenization_bert_generation', 'tokenization_bert_japanese', 'tokenization_bert_fast', 'tokenization_bertweet', 'configuration_blenderbot', 'modeling_blenderbot', 'tokenization_blenderbot', 'configuration_ctrl', 'modeling_ctrl', 'tokenization_ctrl', 'configuration_camembert', 'modeling_camembert', 'tokenization_camembert', 'tokenization_camembert_fast', 'configuration_dpr', 'modeling_dpr', 'tokenization_dpr', 'tokenization_dpr_fast', 'configuration_deberta', 'modeling_deberta', 'tokenization_deberta', 'configuration_distilbert', 'modeling_distilbert', 'tokenization_distilbert', 'tokenization_distilbert_fast', 'configuration_electra', 'modeling_electra', 'tokenization_electra', 'tokenization_electra_fast', 'configuration_encoder_decoder', 'modeling_encoder_decoder', 'configuration_fsmt', 'modeling_fsmt', 'tokenization_fsmt', 'configuration_flaubert', 'modeling_flaubert', 'tokenization_flaubert', 'modeling_funnel', 'configuration_funnel', 'tokenization_funnel', 'tokenization_funnel_fast', 'configuration_gpt2', 'modeling_gpt2', 'tokenization_gpt2', 'tokenization_gpt2_fast', 'tokenization_herbert', 'tokenization_herbert_fast', 'configuration_layoutlm', 'modeling_layoutlm', 'tokenization_layoutlm', 'tokenization_layoutlm_fast', 'configuration_longformer', 'modeling_longformer', 'tokenization_longformer', 'tokenization_longformer_fast', 'configuration_lxmert', 'modeling_lxmert', 'tokenization_lxmert', 'tokenization_lxmert_fast', 'configuration_mbart', 'modeling_mbart', 'tokenization_mbart', 'tokenization_mbart_fast', 'configuration_mmbt', 'modeling_mmbt', 'configuration_mt5', 'modeling_mt5', 'configuration_marian', 'modeling_marian', 'tokenization_marian', 'configuration_mobilebert', 'modeling_mobilebert', 'tokenization_mobilebert', 'tokenization_mobilebert_fast', 'configuration_openai', 'modeling_openai', 'tokenization_openai', 'tokenization_openai_fast', 'configuration_pegasus', 'modeling_pegasus', 'tokenization_pegasus', 'tokenization_pegasus_fast', 'tokenization_phobert', 'configuration_prophetnet', 'modeling_prophetnet', 'tokenization_prophetnet', 'configuration_rag', 'modeling_rag', 'tokenization_rag', 'modeling_reformer', 'configuration_reformer', 'tokenization_reformer', 'tokenization_reformer_fast', 'configuration_retribert', 'modeling_retribert', 'tokenization_retribert', 'tokenization_retribert_fast', 'configuration_roberta', 'modeling_roberta', 'tokenization_roberta', 'tokenization_roberta_fast', 'configuration_squeezebert', 'modeling_squeezebert', 'tokenization_squeezebert', 'tokenization_squeezebert_fast', 'configuration_t5', 'modeling_t5', 'tokenization_t5', 'tokenization_t5_fast', 'modeling_tf_transfo_xl', 'modeling_tf_albert', 'modeling_tf_auto', 'modeling_tf_bart', 'modeling_tf_bert', 'modeling_tf_blenderbot', 'modeling_tf_ctrl', 'modeling_tf_camembert', 'modeling_tf_dpr', 'modeling_tf_distilbert', 'modeling_tf_electra', 'modeling_tf_flaubert', 'modeling_tf_funnel', 'modeling_tf_gpt2', 'modeling_tf_longformer', 'modeling_tf_lxmert', 'modeling_tf_mbart', 'modeling_tf_mt5', 'modeling_tf_marian', 'modeling_tf_mobilebert', 'modeling_tf_openai', 'modeling_tf_pegasus', 'modeling_tf_roberta', 'modeling_tf_t5', 'modeling_tf_xlm', 'modeling_tf_xlm_roberta', 'modeling_tf_xlnet', 'configuration_transfo_xl', 'tokenization_transfo_xl', 'configuration_xlm', 'modeling_xlm', 'configuration_xlm_prophetnet', 'modeling_xlm_prophetnet', 'tokenization_xlm_prophetnet', 'configuration_xlm_roberta', 'modeling_xlm_roberta', 'tokenization_xlm_roberta', 'tokenization_xlm_roberta_fast', 'tokenization_xlm', 'configuration_xlnet', 'modeling_xlnet', 'tokenization_xlnet', 'tokenization_xlnet_fast']\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "display_df(mh._df.head())\n",
    "\n",
    "print(list(mh._df.model_task.unique()))\n",
    "print('')\n",
    "print(list(mh._df.functional_area.unique()))\n",
    "print('')\n",
    "print(list(mh._df.module_part_2.unique()))\n",
    "print('')\n",
    "print(list(mh._df.module_part_3.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide global helper constant\n",
    "\n",
    "Users of this library can simply use `BLURR_MODEL_HELPER` to access all the `ModelHelper` capabilities without having to fetch an instance themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "BLURR_MODEL_HELPER = ModelHelper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can get at the core huggingface objects you need to work with ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... the ***task***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"ModelHelper.get_tasks\" class=\"doc_header\"><code>ModelHelper.get_tasks</code><a href=\"__main__.py#L75\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>ModelHelper.get_tasks</code>(**`arch`**=*`None`*)\n\nGet the type of tasks for which there is a custom model for (*optional: by architecture*). \nThere are a number of customized models built for specific tasks like token classification, \nquestion/answering, LM, etc....",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CausalLM', 'Classification', 'ConditionalGeneration', 'Generation', 'LMHead', 'LMHeadModel', 'MaskedLM', 'MultipleChoice', 'NextSentencePrediction', 'PreTraining', 'QuestionAnswering', 'QuestionAnsweringSimple', 'Seq2SeqLM', 'SequenceClassification', 'TokenClassification']\n\n['ConditionalGeneration', 'QuestionAnswering', 'SequenceClassification']\n"
     ]
    }
   ],
   "source": [
    "print(BLURR_MODEL_HELPER.get_tasks())\n",
    "print('')\n",
    "print(BLURR_MODEL_HELPER.get_tasks('bart'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... the ***architecture***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"ModelHelper.get_architectures\" class=\"doc_header\"><code>ModelHelper.get_architectures</code><a href=\"__main__.py#L53\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>ModelHelper.get_architectures</code>()\n\nUsed to get all the architectures supported by your `Transformers` install",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_architectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['albert', 'albert_fast', 'auto', 'bart', 'bart_fast', 'bert', 'bert_fast', 'bert_generation', 'bert_japanese', 'bertweet', 'blenderbot', 'camembert', 'camembert_fast', 'ctrl', 'deberta', 'distilbert', 'distilbert_fast', 'dpr', 'dpr_fast', 'electra', 'electra_fast', 'encoder_decoder', 'flaubert', 'fsmt', 'funnel', 'funnel_fast', 'gpt2', 'gpt2_fast', 'herbert', 'herbert_fast', 'layoutlm', 'layoutlm_fast', 'longformer', 'longformer_fast', 'lxmert', 'lxmert_fast', 'marian', 'mbart', 'mbart_fast', 'mmbt', 'mobilebert', 'mobilebert_fast', 'mt5', 'openai', 'openai_fast', 'pegasus', 'pegasus_fast', 'phobert', 'prophetnet', 'rag', 'reformer', 'reformer_fast', 'retribert', 'retribert_fast', 'roberta', 'roberta_fast', 'squeezebert', 'squeezebert_fast', 't5', 't5_fast', 'tf_albert', 'tf_auto', 'tf_bart', 'tf_bert', 'tf_blenderbot', 'tf_camembert', 'tf_ctrl', 'tf_distilbert', 'tf_dpr', 'tf_electra', 'tf_flaubert', 'tf_funnel', 'tf_gpt2', 'tf_longformer', 'tf_lxmert', 'tf_marian', 'tf_mbart', 'tf_mobilebert', 'tf_mt5', 'tf_openai', 'tf_pegasus', 'tf_roberta', 'tf_t5', 'tf_transfo_xl', 'tf_xlm', 'tf_xlm_roberta', 'tf_xlnet', 'transfo_xl', 'xlm', 'xlm_prophetnet', 'xlm_roberta', 'xlm_roberta_fast', 'xlnet', 'xlnet_fast']\n"
     ]
    }
   ],
   "source": [
    "print(BLURR_MODEL_HELPER.get_architectures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"ModelHelper.get_model_architecture\" class=\"doc_header\"><code>ModelHelper.get_model_architecture</code><a href=\"__main__.py#L104\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>ModelHelper.get_model_architecture</code>(**`model_name_or_enum`**)\n\nGet the architecture for a given model name / enum",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_model_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta\n"
     ]
    }
   ],
   "source": [
    "print(BLURR_MODEL_HELPER.get_model_architecture('RobertaForSequenceClassification'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... the ***config*** for that particular task and architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"ModelHelper.get_config\" class=\"doc_header\"><code>ModelHelper.get_config</code><a href=\"__main__.py#L59\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>ModelHelper.get_config</code>(**`arch`**)\n\nUsed the locate the name of the configuration class for a given architecture",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n"
     ]
    }
   ],
   "source": [
    "print(BLURR_MODEL_HELPER.get_config('bert'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... the available ***tokenizers*** for that architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"ModelHelper.get_tokenizers\" class=\"doc_header\"><code>ModelHelper.get_tokenizers</code><a href=\"__main__.py#L66\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>ModelHelper.get_tokenizers</code>(**`arch`**)\n\nUsed to get the available huggingface tokenizers for a given architecture. Note: There may be \nmultiple tokenizers and so this returns a list.",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'transformers.models.electra.tokenization_electra.ElectraTokenizer'>]\n"
     ]
    }
   ],
   "source": [
    "print(BLURR_MODEL_HELPER.get_tokenizers('electra'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and lastly the ***models*** (optionally for a given task and/or architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"ModelHelper.get_models\" class=\"doc_header\"><code>ModelHelper.get_models</code><a href=\"__main__.py#L85\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>ModelHelper.get_models</code>(**`arch`**=*`None`*, **`task`**=*`None`*)\n\nThe transformer models available for use (optional: by architecture | task)",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'transformers.models.transfo_xl.modeling_transfo_xl.AdaptiveEmbedding'>, <class 'transformers.models.albert.modeling_albert.AlbertForMaskedLM'>, <class 'transformers.models.albert.modeling_albert.AlbertForMultipleChoice'>, <class 'transformers.models.albert.modeling_albert.AlbertForPreTraining'>, <class 'transformers.models.albert.modeling_albert.AlbertForQuestionAnswering'>]\n"
     ]
    }
   ],
   "source": [
    "print(L(BLURR_MODEL_HELPER.get_models())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>, <class 'transformers.models.bert.modeling_bert.BertForMultipleChoice'>, <class 'transformers.models.bert.modeling_bert.BertForNextSentencePrediction'>, <class 'transformers.models.bert.modeling_bert.BertForPreTraining'>, <class 'transformers.models.bert.modeling_bert.BertForQuestionAnswering'>]\n"
     ]
    }
   ],
   "source": [
    "print(BLURR_MODEL_HELPER.get_models(arch='bert')[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'transformers.models.albert.modeling_albert.AlbertForTokenClassification'>, <class 'transformers.models.auto.modeling_auto.AutoModelForTokenClassification'>, <class 'transformers.models.bert.modeling_bert.BertForTokenClassification'>, <class 'transformers.models.camembert.modeling_camembert.CamembertForTokenClassification'>, <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForTokenClassification'>]\n"
     ]
    }
   ],
   "source": [
    "print(BLURR_MODEL_HELPER.get_models(task='TokenClassification')[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'transformers.models.bert.modeling_bert.BertForTokenClassification'>]\n"
     ]
    }
   ],
   "source": [
    "print(BLURR_MODEL_HELPER.get_models(arch='bert', task='TokenClassification'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some helpful enums to make it easier to get at the *architecture and task* you're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "HF_ARCHITECTURES = Enum('HF_ARCHITECTURES', BLURR_MODEL_HELPER.get_architectures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<HF_ARCHITECTURES.albert: 1>, <HF_ARCHITECTURES.albert_fast: 2>, <HF_ARCHITECTURES.auto: 3>, <HF_ARCHITECTURES.bart: 4>, <HF_ARCHITECTURES.bart_fast: 5>]\n"
     ]
    }
   ],
   "source": [
    "print(L(HF_ARCHITECTURES)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "HF_TASKS_ALL = Enum('HF_TASKS_ALL', BLURR_MODEL_HELPER.get_tasks())\n",
    "HF_TASKS_AUTO = Enum('HF_TASKS_AUTO', BLURR_MODEL_HELPER.get_tasks('auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- all tasks ---\n[<HF_TASKS_ALL.CausalLM: 1>, <HF_TASKS_ALL.Classification: 2>, <HF_TASKS_ALL.ConditionalGeneration: 3>, <HF_TASKS_ALL.Generation: 4>, <HF_TASKS_ALL.LMHead: 5>, <HF_TASKS_ALL.LMHeadModel: 6>, <HF_TASKS_ALL.MaskedLM: 7>, <HF_TASKS_ALL.MultipleChoice: 8>, <HF_TASKS_ALL.NextSentencePrediction: 9>, <HF_TASKS_ALL.PreTraining: 10>, <HF_TASKS_ALL.QuestionAnswering: 11>, <HF_TASKS_ALL.QuestionAnsweringSimple: 12>, <HF_TASKS_ALL.Seq2SeqLM: 13>, <HF_TASKS_ALL.SequenceClassification: 14>, <HF_TASKS_ALL.TokenClassification: 15>]\n\n--- auto only ---\n[<HF_TASKS_AUTO.CausalLM: 1>, <HF_TASKS_AUTO.LMHead: 2>, <HF_TASKS_AUTO.MaskedLM: 3>, <HF_TASKS_AUTO.MultipleChoice: 4>, <HF_TASKS_AUTO.NextSentencePrediction: 5>, <HF_TASKS_AUTO.PreTraining: 6>, <HF_TASKS_AUTO.QuestionAnswering: 7>, <HF_TASKS_AUTO.Seq2SeqLM: 8>, <HF_TASKS_AUTO.SequenceClassification: 9>, <HF_TASKS_AUTO.TokenClassification: 10>]\n"
     ]
    }
   ],
   "source": [
    "print('--- all tasks ---')\n",
    "print(L(HF_TASKS_ALL))\n",
    "print('\\n--- auto only ---')\n",
    "print(L(HF_TASKS_AUTO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HF_TASKS_ALL.Classification: 2>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HF_TASKS_ALL.Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BLURR_MODEL_HELPER.get_classes_for_model` can be used to get the config, tokenizer, and model *classes* you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"ModelHelper.get_classes_for_model\" class=\"doc_header\"><code>ModelHelper.get_classes_for_model</code><a href=\"__main__.py#L94\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>ModelHelper.get_classes_for_model</code>(**`model_name_or_cls`**)\n\nGet tokenizers, config, and model for a given model name / class",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_classes_for_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.roberta.configuration_roberta.RobertaConfig'>\n<class 'transformers.models.roberta.tokenization_roberta.RobertaTokenizer'>\n<class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "config, tokenizers, model = BLURR_MODEL_HELPER.get_classes_for_model('RobertaForSequenceClassification')\n",
    "\n",
    "print(config)\n",
    "print(tokenizers[0])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.distilbert.configuration_distilbert.DistilBertConfig'>\n<class 'transformers.models.distilbert.tokenization_distilbert.DistilBertTokenizer'>\n<class 'transformers.models.distilbert.modeling_distilbert.DistilBertModel'>\n"
     ]
    }
   ],
   "source": [
    "config, tokenizers, model = BLURR_MODEL_HELPER.get_classes_for_model(DistilBertModel)\n",
    "\n",
    "print(config)\n",
    "print(tokenizers[0])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for loading pre-trained (configs, tokenizer, model) hugginface objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "<h4 id=\"ModelHelper.get_hf_objects\" class=\"doc_header\"><code>ModelHelper.get_hf_objects</code><a href=\"__main__.py#L109\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n\n> <code>ModelHelper.get_hf_objects</code>(**`pretrained_model_name_or_path`**, **`task`**=*`None`*, **`config`**=*`None`*, **`tokenizer_cls`**=*`None`*, **`model_cls`**=*`None`*, **`config_kwargs`**=*`{}`*, **`tokenizer_kwargs`**=*`{}`*, **`model_kwargs`**=*`{}`*, **`cache_dir`**=*`None`*)\n\nReturns the architecture (str), config (obj), tokenizer (obj), and model (obj) given at minimum a\n`pre-trained model name or path`. Specify a `task` to ensure the right \"AutoModelFor<task>\" is used to\ncreate the model.\n\nOptionally, you can pass a config (obj), tokenizer (class), and/or model (class) (along with any \nrelated kwargs for each) to get as specific as you want w/r/t what huggingface objects are returned.",
      "text/plain": "<IPython.core.display.Markdown object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ModelHelper(ModelHelper).get_hf_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n<class 'transformers.models.bert.configuration_bert.BertConfig'>\n<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n<class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>\n"
     ]
    }
   ],
   "source": [
    "arch, config, tokenizer, model = BLURR_MODEL_HELPER.get_hf_objects(\"bert-base-cased-finetuned-mrpc\",\n",
    "                                                                   task=HF_TASKS_AUTO.MaskedLM)\n",
    "\n",
    "print(arch)\n",
    "print(type(config))\n",
    "print(type(tokenizer))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flaubert\n<class 'transformers.models.flaubert.tokenization_flaubert.FlaubertTokenizer'>\n<class 'transformers.models.flaubert.configuration_flaubert.FlaubertConfig'>\n<class 'transformers.models.flaubert.modeling_flaubert.FlaubertForQuestionAnsweringSimple'>\n"
     ]
    }
   ],
   "source": [
    "arch, tokenizer, config, model = BLURR_MODEL_HELPER.get_hf_objects(\"fmikaelian/flaubert-base-uncased-squad\",\n",
    "                                                                   task=HF_TASKS_AUTO.QuestionAnswering)\n",
    "\n",
    "print(arch)\n",
    "print(type(config))\n",
    "print(type(tokenizer))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>\n<class 'transformers.models.bert.configuration_bert.BertConfig'>\n<class 'transformers.models.bert.modeling_bert.BertForNextSentencePrediction'>\n"
     ]
    }
   ],
   "source": [
    "arch, tokenizer, config, model = BLURR_MODEL_HELPER.get_hf_objects(\"bert-base-cased-finetuned-mrpc\",\n",
    "                                                                   config=None,\n",
    "                                                                   tokenizer_cls=BertTokenizer, \n",
    "                                                                   model_cls=BertForNextSentencePrediction)\n",
    "print(arch)\n",
    "print(type(config))\n",
    "print(type(tokenizer))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils.ipynb.\n",
      "Converted 01_data-core.ipynb.\n",
      "Converted 01a_data-token-classification.ipynb.\n",
      "Converted 01b_data-question-answering.ipynb.\n",
      "Converted 01za_data-text2text-core.ipynb.\n",
      "Converted 01zb_data-text2text-language-modeling.ipynb.\n",
      "Converted 01zc_data-text2text-summarization.ipynb.\n",
      "Converted 02_modeling-core.ipynb.\n",
      "Converted 02a_modeling-token-classification.ipynb.\n",
      "Converted 02b_modeling-question-answering.ipynb.\n",
      "Converted 02za_modeling-text2text-core.ipynb.\n",
      "Converted 02zb_modeling-text2text-language-modeling.ipynb.\n",
      "Converted 02zc_modeling-text2text-summarization.ipynb.\n",
      "Converted 99a_examples-multilabel.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('py385')",
   "metadata": {
    "interpreter": {
     "hash": "03b2c8230f2ecf2192da6428f7f5e434a1ef0aefb54f65f293ba2ec59df1c4b3"
    }
   },
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
